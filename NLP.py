# # -*- coding: utf-8 -*-
# """NLP_Project.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1aTmGVWlYFJPvRT_pO-i6A22pZBgQuint
# """

# !pip install transformers torch torchvision torchaudio datasets pillow streamlit

# """## **TEXT SUMMARIZATION**"""

# from transformers import BartForConditionalGeneration, BartTokenizer
# import torch

# # Load pre-trained model and tokenizer
# model_name = "facebook/bart-large-cnn"
# tokenizer = BartTokenizer.from_pretrained(model_name)
# model = BartForConditionalGeneration.from_pretrained(model_name)

# # Set the model to evaluation mode
# model.eval()

# def summarize(text, model, tokenizer, max_length=130, min_length=30, do_sample=False):
#     inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
#     summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)
#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
#     return summary

# # Sample input text
# text = """
# Data Science and Data Engineering are two crucial disciplines in the field of Artificial Intelligence and Big Data analytics.
# A Data Scientist primarily focuses on analyzing data, building machine learning models, and deriving insights to solve business problems.
# They use statistical techniques, programming languages like Python and R, and machine learning frameworks such as TensorFlow and PyTorch.
# On the other hand, Data Engineers focus on designing, building, and maintaining the data pipelines that enable the collection, storage, and processing of large-scale datasets.
# They work with tools like Apache Spark, Hadoop, and SQL databases to ensure data is properly structured and accessible for analysis.
# While Data Scientists focus on extracting insights, Data Engineers ensure that the infrastructure is in place to support data-driven decision-making.
# Companies today require both roles to work together to maximize the value of data and drive innovation.
# The demand for skilled Data Scientists and Data Engineers continues to grow as industries increasingly rely on data-driven strategies.
# """

# # Perform summarization
# summary = summarize(text, model, tokenizer)
# print("Original text:\n", text)
# print("\nSummary:\n", summary)

# """## **NEXT WORD PREDICTION**"""

# import torch
# from transformers import GPT2LMHeadModel, GPT2Tokenizer

# # Load the tokenizer and model
# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# model = GPT2LMHeadModel.from_pretrained('gpt2')

# # Set the model to evaluation mode
# model.eval()

# def predict_next_word(prompt, model, tokenizer, top_k=5):
#     # Tokenize input text
#     inputs = tokenizer(prompt, return_tensors='pt')

#     # Generate predictions
#     with torch.no_grad():
#         outputs = model(**inputs)

#     # Get logits of the last token in the sequence
#     next_token_logits = outputs.logits[:, -1, :]

#     # Filter the top k tokens by their probability
#     top_k_tokens = torch.topk(next_token_logits, top_k).indices[0].tolist()

#     # Decode the top k tokens to their corresponding words
#     predicted_tokens = [tokenizer.decode([token]) for token in top_k_tokens]

#     return predicted_tokens

# # Example usage
# prompt = "Data science is an important field because"
# predicted_words = predict_next_word(prompt, model, tokenizer)
# print(f"Prompt: {prompt}")
# print(f"Next word predictions: {predicted_words}")

# """## **STORY PREDICTION**"""

# import torch
# from transformers import GPT2Tokenizer, GPT2LMHeadModel

# def generate_story(prompt, model, tokenizer, max_length=100, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2, do_sample=True):
#     # Tokenize input text
#     inputs = tokenizer(prompt, return_tensors="pt")

#     # Generate text with additional parameters
#     with torch.no_grad():
#         output = model.generate(
#             **inputs,
#             max_length=max_length,
#             temperature=temperature,
#             top_k=top_k,
#             top_p=top_p,  # Nucleus sampling to reduce repetition
#             repetition_penalty=repetition_penalty,  # Penalize repeated words
#             do_sample=do_sample,
#             pad_token_id=tokenizer.eos_token_id
#         )

#     # Decode generated text
#     story = tokenizer.decode(output[0], skip_special_tokens=True)

#     return story

# # Example usage
# prompt = "A team of Data Science Engineers work for"
# generated_story = generate_story(prompt, model, tokenizer)
# print(generated_story)

# """## **SENTIMENT ANALYSIS**"""

# from transformers import AutoModelForSequenceClassification, AutoTokenizer
# import torch

# # Load model and tokenizer
# model_name = "distilbert-base-uncased-finetuned-sst-2-english"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSequenceClassification.from_pretrained(model_name)

# # Set the model to evaluation mode
# model.eval()

# # Define function for sentiment analysis
# def analyze_sentiment(text, model, tokenizer):
#     inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

#     with torch.no_grad():
#         outputs = model(**inputs)

#     logits = outputs.logits
#     predicted_class = torch.argmax(logits, dim=1).item()
#     sentiment = "Positive" if predicted_class == 1 else "Negative"

#     return sentiment

# # Example Usage
# text_1 = "I love this project! It's amazing."
# print("Sentiment:", analyze_sentiment(text_1, model, tokenizer))

# # Example Usage
# text_2 = "I am not happy with your performance."
# print("Sentiment:", analyze_sentiment(text_2, model, tokenizer))

# """## **QUESTION ANSWERING**"""

# from transformers import AutoTokenizer, AutoModelForQuestionAnswering
# import torch

# # Load the pre-trained model and tokenizer
# model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# def answer_question(question, context):
#     # Tokenize input question and context
#     inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")

#     # Get model output
#     outputs = model(**inputs)
#     answer_start_scores = outputs.start_logits
#     answer_end_scores = outputs.end_logits

#     # Get the most likely start and end of the answer
#     answer_start = torch.argmax(answer_start_scores)
#     answer_end = torch.argmax(answer_end_scores) + 1

#     # Ensure answer is valid
#     if answer_start >= answer_end:
#         return "I couldn't find a relevant answer in the given context."

#     # Convert tokens to the answer
#     answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))

#     return answer.strip()

# # Define a context and a question
# context = """
# In the next few decades, artificial intelligence (AI) and quantum computing will revolutionize industries, enhancing automation and decision-making processes.
# AI-powered assistants will become more human-like, helping individuals in their daily lives with real-time problem-solving.
# Quantum computers will surpass classical machines, solving complex problems in seconds that would take years for today's supercomputers.
# With advancements in biotechnology, personalized medicine will allow treatments to be tailored to individual genetic profiles, significantly improving healthcare.
# The rise of autonomous transportation will reduce traffic congestion and enhance safety.
# Moreover, space exploration will expand beyond Mars, with AI-driven robotics assisting astronauts on deep-space missions.
# """

# question = "What role will AI-driven robotics play in space exploration?"

# # Get the answer
# answer = answer_question(question, context)
# print(f"Question: {question}")
# print(f"Answer: {answer}")

# Question_2 = "How will quantum computers impact problem-solving in the future?"
# answer_2 = answer_question(Question_2, context)
# print(f"Question: {Question_2}")
# print(f"Answer: {answer_2}")

# """## **IMAGE GENERATION**"""

# from diffusers import StableDiffusionPipeline
# import torch
# from IPython.display import display

# # Load the pre-trained Stable Diffusion model
# model_id = "CompVis/stable-diffusion-v1-4"
# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

# def generate_image(prompt, pipe):
#     with torch.no_grad():
#         image = pipe(prompt).images[0]
#     return image

# # Example usage
# prompt = "A vibrant autumn forest with falling leaves"
# image = generate_image(prompt,pipe)

# # Display the image correctly in Colab
# display(image)

# # Example usage
# prompt_2 = "A cute cat sleeping on a sofa"
# image_2 = generate_image(prompt_2,pipe)

# # Display the image correctly in Colab
# display(image_2)


from transformers import (BartForConditionalGeneration, BartTokenizer, GPT2LMHeadModel, GPT2Tokenizer, 
                          AutoModelForSequenceClassification, AutoTokenizer,  AutoModelForQuestionAnswering)
import torch

class NLPTool:
    def __init__(self):
        """Initialize and load all necessary models and tokenizers."""
        self.tokenizer_bart = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
        self.model_bart = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
        
        self.tokenizer_gpt2 = GPT2Tokenizer.from_pretrained("gpt2")
        self.model_gpt2 = GPT2LMHeadModel.from_pretrained("gpt2")
        
        self.tokenizer_sent = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        self.model_sent = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        
        self.tokenizer_qa = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
        self.model_qa = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
    
    def summarize_text(self, text, max_length=130, min_length=30):
        """Generate a summary for the input text."""
        inputs = self.tokenizer_bart.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = self.model_bart.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)
        return self.tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True)

    def predict_next_word(self, prompt, top_k=5):
        """Predict the next word based on the given prompt."""
        inputs = self.tokenizer_gpt2(prompt, return_tensors='pt')
        with torch.no_grad():
            outputs = self.model_gpt2(**inputs)
        top_k_tokens = torch.topk(outputs.logits[:, -1, :], top_k).indices[0].tolist()
        return [self.tokenizer_gpt2.decode([token]) for token in top_k_tokens]

    def generate_story(self, prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2):
        """Generate a story based on the given prompt."""
        inputs = self.tokenizer_gpt2(prompt, return_tensors="pt")
        with torch.no_grad():
            output = self.model_gpt2.generate(
                **inputs, max_length=max_length, temperature=temperature,
                top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty,
                do_sample=True, pad_token_id=self.tokenizer_gpt2.eos_token_id
            )
        return self.tokenizer_gpt2.decode(output[0], skip_special_tokens=True)

    def analyze_sentiment(self, text):
        """Analyze sentiment of the given text."""
        inputs = self.tokenizer_sent(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = self.model_sent(**inputs)
        return "Positive" if torch.argmax(outputs.logits, dim=1).item() == 1 else "Negative"

    def answer_question(self, question, context):
        """Answer a question based on the given context."""
        inputs = self.tokenizer_qa.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model_qa(**inputs)
        answer_start, answer_end = torch.argmax(outputs.start_logits), torch.argmax(outputs.end_logits) + 1
        if answer_start >= answer_end:
            return "I couldn't find a relevant answer."
        return self.tokenizer_qa.convert_tokens_to_string(
            self.tokenizer_qa.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
        ).strip()


# Example NLP tasks
if __name__ == "__main__":
    nlp_tool = NLPTool()
    
    text = """Data Science and Data Engineering are two crucial disciplines in the field of Artificial Intelligence and Big Data analytics.
              A Data Scientist primarily focuses on analyzing data, building machine learning models, and deriving insights to solve 
              business problems. They use statistical techniques, programming languages like Python and R, and machine learning frameworks 
              such as TensorFlow and PyTorch. On the other hand, Data Engineers focus on designing, building, and maintaining the data pipelines 
              that enable the collection, storage, and processing of large-scale datasets. They work with tools like Apache Spark, Hadoop, 
              and SQL databases to ensure data is properly structured and accessible for analysis. While Data Scientists focus on extracting
              insights, Data Engineers ensure that the infrastructure is in place to support data-driven decision-making. Companies today 
              require both roles to work together to maximize the value of data and drive innovation. The demand for skilled Data Scientists 
              and Data Engineers continues to grow as industries increasingly rely on data-driven strategies."""
    print("\n\nText:  ",text,"\n\nSummary: \n", nlp_tool.summarize_text(text))
    
    prompt = "Data science is an important field because"
    print("\n\nprompt:  ",prompt,"\n\nNext Word Generation :", nlp_tool.predict_next_word(prompt))
    
    story_prompt = "A team of Data Science Engineers work for"
    print("\n\nStory:  ",story_prompt,"\n\nStory Prediction :\n", nlp_tool.generate_story(story_prompt))
    
    sentiment_text = "I am not happy with your performance."
    print("\n\nSentiment:  ",sentiment_text,"\n\nSentiment Analysis :", nlp_tool.analyze_sentiment(sentiment_text))
    
    context = """In the next few decades, artificial intelligence (AI) and quantum computing will revolutionize industries, 
                enhancing automation and decision-making processes. AI-powered assistants will become more human-like, helping individuals 
                in their daily lives with real-time problem-solving. Quantum computers will surpass classical machines, solving complex problems
                in seconds that would take years for today's supercomputers. With advancements in biotechnology, personalized medicine will 
                allow treatments to be tailored to individual genetic profiles, significantly improving healthcare. The rise of autonomous 
                transportation will reduce traffic congestion and enhance safety. Moreover, space exploration will expand beyond Mars, 
                with AI-driven robotics assisting astronauts on deep-space missions."""
    
    Question = "How will quantum computers impact problem-solving in the future?"
    print("\n\nContext:  ",context,"\n\nQuestion:  ",Question,"\n\nAnswer: ", nlp_tool.answer_question(Question, context))
 